# ì±„ìš©ê³µê³  í¬ë¡¤ëŸ¬ ì—ì´ì „íŠ¸ (Job Posting Crawler Agent)

í•œêµ­ ì£¼ìš” ê¸°ì—…ë“¤ì˜ ì±„ìš©ê³µê³ ë¥¼ ìë™ìœ¼ë¡œ ìˆ˜ì§‘í•˜ê³ , ë©”íƒ€ë°ì´í„°ì™€ ì›¹í˜ì´ì§€ ìŠ¤í¬ë¦°ìƒ·ì„ ì €ì¥í•˜ëŠ” ë¹„ë™ê¸° ì›¹ í¬ë¡¤ëŸ¬ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

**ì™„ì „ ë¹„ë™ê¸° ì²˜ë¦¬** | **CloudFlare ìš°íšŒ** | **PNG ì´ë¯¸ì§€ ìº¡ì²˜** | **HTML/JSON ë©”íƒ€ë°ì´í„° ì €ì¥**

## ì£¼ìš” íŠ¹ì§•

### ë¹„ë™ê¸° ì•„í‚¤í…ì²˜
- **AsyncPlaywrightOrchestrator**: ì™„ì „ ë¹„ë™ê¸° ì²˜ë¦¬ë¡œ ë†’ì€ ë™ì‹œì„± ì§€ì›
- **Playwright ê¸°ë°˜**: ë™ì  í˜ì´ì§€ ë Œë”ë§ ë° JavaScript ì‹¤í–‰ ê°€ëŠ¥
- **CloudFlare ìš°íšŒ**: playwright-stealth + cloudscraperë¡œ ë³´ì•ˆ ìš°íšŒ

### í”ŒëŸ¬ê·¸ì¸ ì•„í‚¤í…ì²˜
- **Registry íŒ¨í„´**: ìƒˆë¡œìš´ íšŒì‚¬ í¬ë¡¤ëŸ¬ë¥¼ ê°„ë‹¨íˆ ì¶”ê°€ ê°€ëŠ¥
- **BaseCrawler**: ëª¨ë“  í¬ë¡¤ëŸ¬ê°€ êµ¬í˜„í•´ì•¼ í•˜ëŠ” ê³µí†µ ì¸í„°í˜ì´ìŠ¤
- **í™•ì¥ì„±**: ìƒˆ íšŒì‚¬ë¥¼ ì¶”ê°€í•  ë•Œ ì½”ì–´ ë¡œì§ ìˆ˜ì • ë¶ˆí•„ìš”

### ë°ì´í„° ì €ì¥ í¬ë§·
- **HTML**: ì›ë³¸ HTML íŒŒì¼ (`data/html/`)
- **JSON**: êµ¬ì¡°í™”ëœ ë©”íƒ€ë°ì´í„° (`data/metadata/`)
- **PNG ì´ë¯¸ì§€**: ì›¹í˜ì´ì§€ ì „ì²´ ìŠ¤í¬ë¦°ìƒ· (`data/screenshots/`)

### AsyncPlaywrightOrchestrator ì›Œí¬í”Œë¡œìš°
```
1. ì±„ìš© ëª©ë¡ í˜ì´ì§€ ì˜¤í”ˆ (Playwright)
   â†“
2. ê°œë³„ ê³µê³  URL ì¶”ì¶œ (í¬ë¡¤ëŸ¬ë³„ë¡œ êµ¬í˜„)
   â†“
3. ê° ê³µê³  ìƒì„¸ ì •ë³´ íŒŒì‹± (ë³‘ë ¬ ì²˜ë¦¬, Semaphoreë¡œ ë™ì‹œì„± ì œì–´)
   â†“
4. HTML ì›ë³¸ ì €ì¥
   â†“
5. JSON ë©”íƒ€ë°ì´í„° ì €ì¥
   â†“
6. PNG ì´ë¯¸ì§€ ìº¡ì²˜ (Playwright - ë³‘ë ¬ ì²˜ë¦¬)
```

### ì£¼ìš” ì»´í¬ë„ŒíŠ¸

#### 1. BaseCrawler (ì¶”ìƒ í´ë˜ìŠ¤)
ëª¨ë“  íšŒì‚¬ í¬ë¡¤ëŸ¬ê°€ êµ¬í˜„í•´ì•¼ í•˜ëŠ” ì¸í„°í˜ì´ìŠ¤:
- `get_company_name()`: íšŒì‚¬ëª… ë°˜í™˜
- `get_job_list_urls()`: ì±„ìš© ëª©ë¡ URL ë¦¬ìŠ¤íŠ¸
- `extract_job_urls(page)`: Playwright pageì—ì„œ ê³µê³  URL ì¶”ì¶œ
- `parse_job_detail(page, url, idx)`: ìƒì„¸ í˜ì´ì§€ íŒŒì‹±
- `get_wait_time()`: í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸° ì‹œê°„
- `get_max_concurrent_jobs()`: ë™ì‹œ ì²˜ë¦¬ ê³µê³  ìˆ˜
- `requires_playwright()`: Playwright ì‚¬ìš© ì—¬ë¶€

#### 2. CrawlerRegistry
í”ŒëŸ¬ê·¸ì¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ íŒ¨í„´ êµ¬í˜„:
```python
registry = CrawlerRegistry()
registry.register(CoupangCrawler())
registry.register(WoowahanCrawler())
registry.get_crawler("Coupang")
```

#### 3. AsyncPlaywrightOrchestrator
ì™„ì „ ë¹„ë™ê¸° í¬ë¡¤ë§ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°:
- Playwright ê¸°ë°˜ ë™ì  í˜ì´ì§€ ì²˜ë¦¬
- Semaphoreë¥¼ ì´ìš©í•œ ë™ì‹œì„± ì œì–´
- ê° í¬ë¡¤ëŸ¬ë³„ë¡œ ìµœì í™”ëœ ë™ì‹œ ì‘ì—… ìˆ˜ ì„¤ì •
- HTML + JSON ë©”íƒ€ë°ì´í„° + PNG ì´ë¯¸ì§€ ë™ì‹œ ì €ì¥
- playwright-stealthë¥¼ í†µí•œ ìë™ ê°ì§€ ìš°íšŒ

**ì£¼ìš” ë©”ì„œë“œ:**
- `crawl_company(company_name, max_jobs)`: íŠ¹ì • íšŒì‚¬ í¬ë¡¤ë§
  - `company_name`: í¬ë¡¤ëŸ¬ì— ë“±ë¡ëœ íšŒì‚¬ëª… (ì˜ˆ: "Coupang", "Woowahan")
  - `max_jobs`: ìµœëŒ€ í¬ë¡¤ë§ ê³µê³  ìˆ˜ (Noneì´ë©´ ì „ì²´)
  - ë°˜í™˜ê°’: í¬ë¡¤ë§ ê²°ê³¼ + ì €ì¥ ê²½ë¡œ ì •ë³´
- `crawl_all_companies(max_jobs)`: ëª¨ë“  íšŒì‚¬ ë³‘ë ¬ í¬ë¡¤ë§
  - ëª¨ë“  ë“±ë¡ëœ íšŒì‚¬ë¥¼ ë™ì‹œì— í¬ë¡¤ë§
  - ê° íšŒì‚¬ëŠ” ë…ë¦½ì ì¸ asyncio íƒœìŠ¤í¬ë¡œ ì‹¤í–‰

#### 4. PlaywrightCaptureAgent
Playwright ê¸°ë°˜ ì›¹í˜ì´ì§€ ìŠ¤í¬ë¦°ìƒ· ìº¡ì²˜:
- **PNG/JPEG í˜•ì‹ ì§€ì›**: PNGëŠ” ê³ í’ˆì§ˆ(~2.4MB), JPEGëŠ” ì••ì¶• í¬ë§·(~300KB)
- **ì „ì²´ í˜ì´ì§€ ìº¡ì²˜**: `full_page=True`ë¡œ ìŠ¤í¬ë¡¤ ì˜ì—­ê¹Œì§€ ëª¨ë‘ ìº¡ì²˜
- **ë™ì  ì½˜í…ì¸  ì²˜ë¦¬**: ìë™ ìŠ¤í¬ë¡¤ë¡œ lazy-loading ì½˜í…ì¸  ë¡œë“œ
- **ë°°ì¹˜ ì²˜ë¦¬**: ì—¬ëŸ¬ URLì„ ë³‘ë ¬ë¡œ íš¨ìœ¨ì ìœ¼ë¡œ ìº¡ì²˜
- **Chrome DevTools Protocol (CDP)**: ì •í™•í•œ ë Œë”ë§ ì§€ì›

**ì£¼ìš” ë©”ì„œë“œ**:
```python
# ë‹¨ì¼ í˜ì´ì§€ ì´ë¯¸ì§€ ìº¡ì²˜
image_bytes = await capture_agent.capture_as_image(
    url="https://...",
    wait_time=5,           # ë¡œë“œ í›„ ëŒ€ê¸°ì‹œê°„ (ì´ˆ)
    image_format="png"     # "png" ë˜ëŠ” "jpeg"
)

# ì—¬ëŸ¬ í˜ì´ì§€ ì¼ê´„ ìº¡ì²˜
results = await capture_agent.capture_as_image_bulk(urls)

# í˜¸í™˜ì„± ìœ ì§€ (ê¶Œì¥í•˜ì§€ ì•ŠìŒ)
image_bytes = await capture_agent.capture_as_pdf(url)  # ë‚´ë¶€ì ìœ¼ë¡œ ì´ë¯¸ì§€ ìº¡ì²˜
```

#### 5. StorageAgent
ë°ì´í„° ì €ì¥ì†Œ ê´€ë¦¬:
- **ì´ë¯¸ì§€ ì €ì¥**: `save_image_locally()` - PNG/JPEG í¬ë§·ìœ¼ë¡œ ë¡œì»¬ ì €ì¥
- **S3 í´ë¼ìš°ë“œ ì €ì¥**: `save_image_to_s3()` - AWS S3ì— ì´ë¯¸ì§€ ì—…ë¡œë“œ
- **HTML ì €ì¥**: ì›ë³¸ ì›¹í˜ì´ì§€ HTML ì €ì¥
- **JSON ì €ì¥**: êµ¬ì¡°í™”ëœ ë©”íƒ€ë°ì´í„° JSON ì €ì¥
- **í•œêµ­ì–´ íŒŒì¼ëª…**: í•œêµ­ì–´ ê³µê³  ì œëª©ì„ í¬í•¨í•œ íŒŒì¼ëª… ì§€ì›

**ì£¼ìš” ë©”ì„œë“œ**:
```python
# ì´ë¯¸ì§€ë¥¼ ë¡œì»¬ê³¼ S3 ëª¨ë‘ ì €ì¥
result = storage_agent.save_image(
    image_bytes=image_data,
    company="KT",
    job_id="kt_232245",
    job_title="ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œì",
    subfolder="2025-11-18",
    image_format="png",        # "png" ë˜ëŠ” "jpeg"
    save_to_s3=True           # S3 ì €ì¥ í¬í•¨
)

# ê²°ê³¼: {
#   "success": True,
#   "local_path": "data/screenshots/KT/2025-11-18/...",
#   "s3_key": "KT/2025-11-18/..."
# }
```

**ì €ì¥ ê²½ë¡œ êµ¬ì¡°**:
```
data/
â”œâ”€â”€ screenshots/          # PNG ì´ë¯¸ì§€ ì €ì¥
â”‚   â””â”€â”€ KT/
â”‚       â””â”€â”€ 2025-11-18/
â”‚           â”œâ”€â”€ kt_232245_ì†Œí”„íŠ¸ì›¨ì–´ê°œë°œì_20251118_110414.png
â”‚           â””â”€â”€ kt_232369_ë°ì´í„°ë¶„ì„ê°€_20251118_110524.png
â”œâ”€â”€ html/                # ì›ë³¸ HTML ì €ì¥
â”‚   â””â”€â”€ KT/2025-11-18/*.html
â”œâ”€â”€ metadata/            # JSON ë©”íƒ€ë°ì´í„°
â”‚   â””â”€â”€ KT/2025-11-18/*_metadata.json
â””â”€â”€ pdfs/                # ë ˆê±°ì‹œ PDF (í˜¸í™˜ì„±)
    â””â”€â”€ KT/2025-11-18/*.pdf
```

#### 6. CloudFlare ìš°íšŒ ì „ëµ
**í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼:**
- **ëª©ë¡ í˜ì´ì§€**: cloudscraperë¡œ JavaScript ì±Œë¦°ì§€ ìš°íšŒ
- **ìƒì„¸ í˜ì´ì§€**: playwright-stealthë¡œ ìë™ ê°ì§€ ìš°íšŒ
- **User-Agent**: ì‹¤ì œ Chrome ë¸Œë¼ìš°ì €ì™€ ë™ì¼í•˜ê²Œ ì„¤ì •
- **Stealth ì ìš©**: ëª¨ë“  Playwright page ì¸ìŠ¤í„´ìŠ¤ì— ì ìš©

## ë¹ ë¥¸ ì‹œì‘ (5ë¶„ ì•ˆì— ì‹œì‘í•˜ê¸°)

### 1. ì €ì¥ì†Œ ë³µì œ ë° ê¸°ë³¸ ì„¤ì •
```bash
# 1. ì´ í”„ë¡œì íŠ¸ í´ë¡ 
git clone https://github.com/your-repo/crawler_agent
cd crawler_agent

# 2. Python ê°€ìƒ í™˜ê²½ ìƒì„±
python -m venv venv

# Windows
venv\Scripts\activate
# macOS/Linux
source venv/bin/activate

# 3. ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt
```

### 2. 5ì¤„ë¡œ í¬ë¡¤ë§ ì‹œì‘í•˜ê¸°
```python
import asyncio
from crawlers import CrawlerRegistry, CoupangCrawler
from async_orchestrator import AsyncPlaywrightOrchestrator

async def main():
    registry = CrawlerRegistry()
    registry.register(CoupangCrawler())
    orchestrator = AsyncPlaywrightOrchestrator(registry=registry)
    result = await orchestrator.crawl_company("Coupang", max_jobs=3)
    print(f"í¬ë¡¤ë§ ì™„ë£Œ! {result['total_jobs']}ê°œ ê³µê³  ì €ì¥")

asyncio.run(main())
```

### 3. ê²°ê³¼ í™•ì¸
```bash
# ì €ì¥ëœ íŒŒì¼ í™•ì¸
ls data/screenshots/Coupang/$(date +%Y-%m-%d)/  # PNG ì´ë¯¸ì§€
ls data/html/Coupang/$(date +%Y-%m-%d)/         # HTML
ls data/metadata/Coupang/$(date +%Y-%m-%d)/     # JSON ë©”íƒ€ë°ì´í„°
```

## ì„¤ì¹˜

### 1. ì˜ì¡´ì„± ì„¤ì¹˜
```bash
pip install -r requirements.txt
```

**ì£¼ìš” ì˜ì¡´ì„±:**
- `playwright`: ì›¹ ë¸Œë¼ìš°ì € ìë™í™”
- `playwright-stealth`: CloudFlare ìš°íšŒ
- `cloudscraper`: CloudFlare JavaScript ì±Œë¦°ì§€ ìš°íšŒ
- `aiohttp`: ë¹„ë™ê¸° HTTP ìš”ì²­
- `beautifulsoup4`: HTML íŒŒì‹±

### 2. Playwright ë¸Œë¼ìš°ì € ì„¤ì¹˜
```bash
playwright install chromium
```

### 3. í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (ì„ íƒì‚¬í•­)
```bash
cp .env.example .env
# .env íŒŒì¼ ìˆ˜ì • (S3, í”„ë¡ì‹œ ë“± ì„¤ì •)
```

## ì‚¬ìš©ë²•

### ê¸°ë³¸ ì‚¬ìš© (AsyncPlaywrightOrchestrator)

```python
import asyncio
from crawlers import CrawlerRegistry, CoupangCrawler, WoowahanCrawler
from async_orchestrator import AsyncPlaywrightOrchestrator
from agents import StorageAgent, PlaywrightCaptureAgent

async def crawl():
    # ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì„¤ì •
    registry = CrawlerRegistry()
    registry.register(CoupangCrawler())
    registry.register(WoowahanCrawler())

    # ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
    storage_agent = StorageAgent()
    playwright_capture_agent = PlaywrightCaptureAgent(headless=True)

    # ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ìƒì„±
    orchestrator = AsyncPlaywrightOrchestrator(
        registry=registry,
        storage_agent=storage_agent,
        playwright_capture_agent=playwright_capture_agent,
        headless=True,
        use_vector_embedding=False,
    )

    # íŠ¹ì • íšŒì‚¬ í¬ë¡¤ë§ (ìµœëŒ€ 5ê°œ ê³µê³ )
    result = await orchestrator.crawl_company("Coupang", max_jobs=5)
    print(f"Total jobs: {result['total_jobs']}")
    print(f"Saved: {result['successful_saves']}")

    # ëª¨ë“  íšŒì‚¬ ë³‘ë ¬ í¬ë¡¤ë§
    all_results = await orchestrator.crawl_all_companies(max_jobs=3)

    return result

# ì‹¤í–‰
asyncio.run(crawl())
```

### ì €ì¥ ê²½ë¡œ êµ¬ì¡°

```
data/
â”œâ”€â”€ html/
â”‚   â”œâ”€â”€ Coupang/
â”‚   â”‚   â””â”€â”€ 2025-11-13/
â”‚   â”‚       â”œâ”€â”€ coupang_001_Software_Engineer.html
â”‚   â”‚       â””â”€â”€ coupang_002_Data_Scientist.html
â”‚   â””â”€â”€ Woowahan/
â”‚       â””â”€â”€ 2025-11-13/
â”‚           â”œâ”€â”€ R2508018_...html
â”‚           â””â”€â”€ R2511004_...html
â”œâ”€â”€ metadata/
â”‚   â”œâ”€â”€ Coupang/
â”‚   â”‚   â””â”€â”€ 2025-11-13/
â”‚   â”‚       â”œâ”€â”€ coupang_001_metadata.json
â”‚   â”‚       â””â”€â”€ coupang_002_metadata.json
â”‚   â””â”€â”€ Woowahan/
â”‚       â””â”€â”€ 2025-11-13/
â”‚           â”œâ”€â”€ R2508018_metadata.json
â”‚           â””â”€â”€ R2511004_metadata.json
â””â”€â”€ pdfs/
    â”œâ”€â”€ Coupang/
    â”‚   â””â”€â”€ 2025-11-13/
    â”‚       â”œâ”€â”€ coupang_001_...20251113_200927.pdf
    â”‚       â””â”€â”€ coupang_002_...20251113_200928.pdf
    â””â”€â”€ Woowahan/
        â””â”€â”€ 2025-11-13/
            â”œâ”€â”€ R2508018_...pdf
            â””â”€â”€ R2511004_...pdf
```

### ê²°ê³¼ JSON í˜•ì‹

```json
{
  "success": true,
  "company_name": "Coupang",
  "total_jobs": 5,
  "successful_saves": 5,
  "failed_saves": 0,
  "job_listings": [
    {
      "url": "https://www.coupang.jobs/kr/jobs/...",
      "job_id": "coupang_123",
      "title": "Software Engineer",
      "company": "Coupang",
      "location": "Seoul, South Korea",
      "posting_date": "2025-11-13",
      "closing_date": "2025-12-13",
      "job_description": "...",
      "metadata": {...}
    }
  ],
  "storage_results": [
    {
      "success": true,
      "job_id": "coupang_123",
      "title": "Software Engineer",
      "html_path": "data/html/Coupang/2025-11-13/...",
      "json_path": "data/metadata/Coupang/2025-11-13/...",
      "pdf_path": "data/pdfs/Coupang/2025-11-13/..."
    }
  ],
  "elapsed_seconds": 65.98,
  "timestamp": "2025-11-13T20:09:35.121000"
}
```

## ìƒˆë¡œìš´ íšŒì‚¬ í¬ë¡¤ëŸ¬ ì¶”ê°€í•˜ê¸°

### 1. ìƒˆ í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤ ì‘ì„±

```python
# crawlers/amazon.py
from typing import List, Dict, Optional, Any
from playwright.async_api import Page
from .base_crawler import BaseCrawler

class AmazonCrawler(BaseCrawler):
    def get_company_name(self) -> str:
        return "Amazon"

    def get_job_list_urls(self) -> List[str]:
        return ["https://amazon.jobs/en/search"]

    async def extract_job_urls(self, page: Page) -> List[Dict[str, str]]:
        """Playwright pageì—ì„œ ê³µê³  URL ì¶”ì¶œ"""
        try:
            job_urls = []
            # JavaScript ì‹¤í–‰ ë˜ëŠ” CSS ì„ íƒìë¡œ URL ì¶”ì¶œ
            links = await page.query_selector_all('a[href*="/jobs/"]')
            for link in links:
                href = await link.get_attribute('href')
                if href:
                    job_urls.append({
                        "url": self._normalize_url(href),
                        "job_id": self._extract_job_id(href),
                        "title": ""
                    })
            return job_urls
        except Exception as e:
            self.logger.error(f"Failed to extract job URLs: {e}")
            return []

    async def parse_job_detail(self, page: Page, url: str, idx: int) -> Optional[Dict[str, Any]]:
        """ìƒì„¸ í˜ì´ì§€ íŒŒì‹±"""
        try:
            await page.goto(url, wait_until='domcontentloaded', timeout=self.get_timeout())
            await asyncio.sleep(self.get_wait_time())

            job_data = {
                "url": url,
                "job_id": self._extract_job_id(url),
                "company": self.get_company_name(),
                "title": "",
                "location": "",
                "job_description": "",
                # ... ê¸°íƒ€ í•„ë“œ
                "metadata": {}
            }

            # JavaScriptë¡œ ë°ì´í„° ì¶”ì¶œ
            result = await page.evaluate("""
                () => {
                    return {
                        title: document.querySelector('h1')?.textContent || '',
                        location: document.querySelector('[class*="location"]')?.textContent || '',
                    };
                }
            """)

            job_data.update(result)
            return job_data
        except Exception as e:
            self.logger.error(f"Failed to parse job detail: {e}")
            return None

    def get_wait_time(self) -> int:
        return 3

    def get_max_concurrent_jobs(self) -> int:
        return 3

    def get_timeout(self) -> int:
        return 30000

    def requires_playwright(self) -> bool:
        return True

    def _normalize_url(self, href: str) -> str:
        if href.startswith("http"):
            return href
        elif href.startswith("/"):
            return "https://amazon.jobs" + href
        else:
            return "https://amazon.jobs/" + href

    def _extract_job_id(self, url: str) -> str:
        # URLì—ì„œ job_id ì¶”ì¶œ ë¡œì§
        job_id = url.rstrip('/').split('/')[-1]
        return f"amazon_{job_id}"
```

### 2. ë ˆì§€ìŠ¤íŠ¸ë¦¬ì— ë“±ë¡

```python
import asyncio
from crawlers import CrawlerRegistry, AmazonCrawler
from async_orchestrator import AsyncPlaywrightOrchestrator

async def main():
    registry = CrawlerRegistry()
    registry.register(AmazonCrawler())

    orchestrator = AsyncPlaywrightOrchestrator(registry=registry)
    result = await orchestrator.crawl_company("Amazon", max_jobs=5)

asyncio.run(main())
```

### 3. í¬ë¡¤ëŸ¬ êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸

- âœ… `get_company_name()`: íšŒì‚¬ëª… ë°˜í™˜
- âœ… `get_job_list_urls()`: ì±„ìš© ëª©ë¡ í˜ì´ì§€ URL ë°˜í™˜
- âœ… `extract_job_urls(page)`: Playwright pageì—ì„œ URL ì¶”ì¶œ
- âœ… `parse_job_detail(page, url, idx)`: ìƒì„¸ í˜ì´ì§€ íŒŒì‹±
- âœ… `get_wait_time()`: í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
- âœ… `get_max_concurrent_jobs()`: ë™ì‹œ ì²˜ë¦¬ ìˆ˜
- âœ… `get_timeout()`: íƒ€ì„ì•„ì›ƒ (ë°€ë¦¬ì´ˆ)
- âœ… `requires_playwright()`: True ë°˜í™˜
- âœ… `_normalize_url()`: ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
- âœ… `_extract_job_id()`: URLì—ì„œ ê³ ìœ  job_id ì¶”ì¶œ

## í”„ë¡œì íŠ¸ êµ¬ì¡°

```
crawler_agent/
â”œâ”€â”€ crawlers/                     # íšŒì‚¬ë³„ í¬ë¡¤ëŸ¬ êµ¬í˜„
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base_crawler.py           # ì¶”ìƒ ê¸°ë³¸ í´ë˜ìŠ¤ (í•„ìˆ˜ ë©”ì„œë“œ ì •ì˜)
â”‚   â”œâ”€â”€ registry.py               # Registry íŒ¨í„´ (í¬ë¡¤ëŸ¬ ê´€ë¦¬)
â”‚   â”œâ”€â”€ coupang.py                # Coupang í¬ë¡¤ëŸ¬ (CloudFlare ìš°íšŒ)
â”‚   â”œâ”€â”€ woowahan.py               # Woowahan í¬ë¡¤ëŸ¬
â”‚   â”œâ”€â”€ kt.py                     # KT í¬ë¡¤ëŸ¬
â”‚   â”œâ”€â”€ naver.py                  # Naver í¬ë¡¤ëŸ¬
â”‚   â””â”€â”€ kakao.py                  # Kakao í¬ë¡¤ëŸ¬
â”œâ”€â”€ agents/                       # ë°ì´í„° ì²˜ë¦¬ ì—ì´ì „íŠ¸
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ playwright_capture.py     # Playwright ê¸°ë°˜ ì´ë¯¸ì§€ ìº¡ì²˜ (PNG/JPEG)
â”‚   â””â”€â”€ storage.py                # ì €ì¥ì†Œ ì—ì´ì „íŠ¸ (ë¡œì»¬/S3)
â”œâ”€â”€ async_orchestrator.py         # ë¹„ë™ê¸° ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° (í¬ë¡¤ë§ ì¡°ìœ¨)
â”œâ”€â”€ orchestrator.py               # ë ˆê±°ì‹œ LangGraph ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°
â”œâ”€â”€ config.py                     # ì„¤ì • ê´€ë¦¬
â”œâ”€â”€ main.py                       # ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ requirements.txt              # Python ì˜ì¡´ì„±
â”œâ”€â”€ .env.example                  # í™˜ê²½ ë³€ìˆ˜ ì˜ˆì œ
â”œâ”€â”€ test_image_capture.py         # ì´ë¯¸ì§€ ìº¡ì²˜ í…ŒìŠ¤íŠ¸
â””â”€â”€ README.md                     # ì´ íŒŒì¼
```

**ì €ì¥ ë””ë ‰í† ë¦¬:**
```
data/
â”œâ”€â”€ screenshots/   # PNG ì´ë¯¸ì§€ (2.4MB/ê°œ) - ë©”ì¸ ì €ì¥ì†Œ
â”œâ”€â”€ html/          # ì›ë³¸ HTML íŒŒì¼
â”œâ”€â”€ metadata/      # JSON ë©”íƒ€ë°ì´í„° (êµ¬ì¡°í™”ëœ ì •ë³´)
â””â”€â”€ pdfs/          # ë ˆê±°ì‹œ PDF íŒŒì¼ (í˜¸í™˜ì„±ìš©)
```

## ì½”ë“œ ì´í•´í•˜ê¸° (ì´ˆë³´ì ê°€ì´ë“œ)

### 1. ì‹¤í–‰ íë¦„ ì´í•´í•˜ê¸°

**í¬ë¡¤ë§ì´ ì‹œì‘ë˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ ìˆœì„œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤:**

```
ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸
    â†“
CrawlerRegistryì—ì„œ í¬ë¡¤ëŸ¬ ë“±ë¡
    â†“
AsyncPlaywrightOrchestrator ìƒì„±
    â†“
crawl_company() í˜¸ì¶œ
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1ï¸âƒ£  ì±„ìš© ëª©ë¡ í˜ì´ì§€ ì—´ê¸°        â”‚
â”‚   (Playwright ì‚¬ìš©)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2ï¸âƒ£  ê³µê³  URL ì¶”ì¶œ               â”‚
â”‚   (í¬ë¡¤ëŸ¬ë³„ ê³ ìœ  ë¡œì§)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3ï¸âƒ£  ê° ê³µê³  ìƒì„¸ í˜ì´ì§€ íŒŒì‹±     â”‚
â”‚   (ë³‘ë ¬ ì²˜ë¦¬, Semaphoreë¡œ ì œì–´)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4ï¸âƒ£  HTML, JSON, PNG ë³‘ë ¬ ì €ì¥   â”‚
â”‚   (ê°ê° ë‹¤ë¥¸ ë””ë ‰í† ë¦¬)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
ê²°ê³¼ ë°˜í™˜ (ì„±ê³µ ì—¬ë¶€, ì €ì¥ ê²½ë¡œ ë“±)
```

### 2. ê° íŒŒì¼ì˜ ì—­í• 

#### `crawlers/base_crawler.py`
**ì—­í• **: ëª¨ë“  í¬ë¡¤ëŸ¬ê°€ ë”°ë¥¼ ê³µí†µ ì¸í„°í˜ì´ìŠ¤ ì •ì˜
**í•µì‹¬ ê°œë…**: ì¶”ìƒ í´ë˜ìŠ¤ (Abstract Base Class)
**ì´ê²ƒì´ í•„ìš”í•œ ì´ìœ **: ìƒˆë¡œìš´ í¬ë¡¤ëŸ¬ë¥¼ ì¶”ê°€í•  ë•Œ ì–´ë–¤ ë©”ì„œë“œë¥¼ êµ¬í˜„í•´ì•¼ í•˜ëŠ”ì§€ ëª…í™•í•¨

```python
# ëª¨ë“  í¬ë¡¤ëŸ¬ëŠ” ì´ í´ë˜ìŠ¤ë¥¼ ìƒì†
class BaseCrawler:
    async def extract_job_urls(self, page):
        """í•„ìˆ˜ êµ¬í˜„: URL ì¶”ì¶œ"""
        pass

# ìƒˆ í¬ë¡¤ëŸ¬ ë§Œë“¤ ë•Œ
class AmazonCrawler(BaseCrawler):
    async def extract_job_urls(self, page):
        # Amazon ë§ì¶¤ ë¡œì§ ì‘ì„±
        pass
```

#### `crawlers/registry.py`
**ì—­í• **: ë“±ë¡ëœ í¬ë¡¤ëŸ¬ë“¤ì„ ê´€ë¦¬í•˜ëŠ” ì €ì¥ì†Œ
**í•µì‹¬ ê°œë…**: Registry íŒ¨í„´ (ë™ì ìœ¼ë¡œ ê°ì²´ ë“±ë¡/ì¡°íšŒ)
**ì´ê²ƒì´ í•„ìš”í•œ ì´ìœ **: ë©”ì¸ ì½”ë“œ ìˆ˜ì • ì—†ì´ ìƒˆ í¬ë¡¤ëŸ¬ ì¶”ê°€ ê°€ëŠ¥

```python
registry = CrawlerRegistry()
registry.register(CoupangCrawler())    # ë“±ë¡
registry.register(WoowahanCrawler())
crawler = registry.get_crawler("Coupang")  # ì¡°íšŒ
```

#### `agents/playwright_capture.py`
**ì—­í• **: ì›¹í˜ì´ì§€ë¥¼ PNG/JPEG ì´ë¯¸ì§€ë¡œ ìº¡ì²˜
**í•µì‹¬ ê°œë…**: ì´ë¯¸ì§€ ìº¡ì²˜ (PDF â†’ PNG/JPEGë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜)
**ì´ê²ƒì´ í•„ìš”í•œ ì´ìœ **: ì›¹í˜ì´ì§€ë¥¼ ì¦ê±° ìë£Œë¡œ ë³´ì¡´

```python
# ë‹¨ì¼ í˜ì´ì§€ ìº¡ì²˜
image_bytes = await capture_agent.capture_as_image(url)

# ì—¬ëŸ¬ í˜ì´ì§€ ì¼ê´„ ìº¡ì²˜
results = await capture_agent.capture_as_image_bulk(urls)
```

#### `agents/storage.py`
**ì—­í• **: ìº¡ì²˜í•œ ì´ë¯¸ì§€, HTML, JSONì„ ì €ì¥ì†Œì— ë³´ê´€
**í•µì‹¬ ê°œë…**: ë‹¤ì¤‘ ì €ì¥ì†Œ ì§€ì› (ë¡œì»¬ + S3)
**ì´ê²ƒì´ í•„ìš”í•œ ì´ìœ **: ìœ ì—°í•œ ì €ì¥ì†Œ ì„ íƒ (ê°œë°œ ì‹œ ë¡œì»¬, í”„ë¡œë•ì…˜ ì‹œ S3)

```python
# ë¡œì»¬ ì €ì¥ì†Œì—ë§Œ ì €ì¥
result = storage.save_image(image_bytes, ..., save_to_s3=False)

# ë¡œì»¬ + S3 ëª¨ë‘ ì €ì¥
result = storage.save_image(image_bytes, ..., save_to_s3=True)
```

#### `async_orchestrator.py`
**ì—­í• **: í¬ë¡¤ë§ ì „ì²´ í”„ë¡œì„¸ìŠ¤ë¥¼ ì¡°ìœ¨
**í•µì‹¬ ê°œë…**: ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° íŒ¨í„´ (ì—¬ëŸ¬ ì—ì´ì „íŠ¸ ì¡°ìœ¨)
**ì´ê²ƒì´ í•„ìš”í•œ ì´ìœ **: ë³µì¡í•œ ì›Œí¬í”Œë¡œìš°ë¥¼ ë‹¨ìˆœí•˜ê²Œ ê´€ë¦¬

```python
# í•œ ì¤„ë¡œ í¬ë¡¤ë§ ì‹œì‘
result = await orchestrator.crawl_company("Coupang", max_jobs=5)
# ë‚´ë¶€ì ìœ¼ë¡œëŠ” URL ì¶”ì¶œ â†’ íŒŒì‹± â†’ ìº¡ì²˜ â†’ ì €ì¥ì„ ëª¨ë‘ ìˆ˜í–‰
```

### 3. ë¹„ë™ê¸° í”„ë¡œê·¸ë˜ë° ì´í•´í•˜ê¸°

**async/awaitê°€ ì¤‘ìš”í•œ ì´ìœ **: ì—¬ëŸ¬ ì›¹í˜ì´ì§€ë¥¼ **ë™ì‹œì—** ì²˜ë¦¬

```python
# âŒ ëŠë¦¼: ìˆœì°¨ ì²˜ë¦¬ (1í˜ì´ì§€ 5ì´ˆ Ã— 10í˜ì´ì§€ = 50ì´ˆ)
for url in urls:
    await crawl_page(url)  # í•˜ë‚˜ ëë‚˜ì•¼ ë‹¤ìŒ ì‹œì‘

# âœ… ë¹ ë¦„: ë³‘ë ¬ ì²˜ë¦¬ (ë™ì‹œì— 10ê°œ ì²˜ë¦¬ = ì•½ 5ì´ˆ)
tasks = [crawl_page(url) for url in urls]
await asyncio.gather(*tasks)  # ëª¨ë‘ ë™ì‹œ ì‹¤í–‰
```

### 4. Semaphoreë¡œ ë™ì‹œ ì²˜ë¦¬ ì œí•œí•˜ê¸°

**ë¬¸ì œ**: ë™ì‹œì— ë„ˆë¬´ ë§ì€ ìš”ì²­ì„ í•˜ë©´ ì„œë²„ê°€ ì°¨ë‹¨ (CloudFlare)

```python
# Semaphore: "ìµœëŒ€ 3ê°œì”©ë§Œ ë™ì‹œ ì²˜ë¦¬"
semaphore = asyncio.Semaphore(3)

async def crawl_with_limit(url):
    async with semaphore:  # ìµœëŒ€ 3ê°œê¹Œì§€ë§Œ ì´ ì½”ë“œ ì‹¤í–‰
        await crawl_page(url)
```

### 5. ì—ëŸ¬ ì²˜ë¦¬ ì´í•´í•˜ê¸°

```python
# í•˜ë‚˜ ì‹¤íŒ¨í•´ë„ ë‹¤ë¥¸ ê²ƒì€ ê³„ì†
tasks = [crawl_page(url) for url in urls]
results = await asyncio.gather(*tasks, return_exceptions=True)
# results: [data1, data2, Exception(...), data4, ...]
```

## í•µì‹¬ ì„¤ê³„ íŒ¨í„´

### 1. Registry íŒ¨í„´
ìƒˆë¡œìš´ íšŒì‚¬ í¬ë¡¤ëŸ¬ë¥¼ ë™ì ìœ¼ë¡œ ë“±ë¡í•˜ê³  ê´€ë¦¬:
```python
registry = CrawlerRegistry()
registry.register(CoupangCrawler())  # ìƒˆ í¬ë¡¤ëŸ¬ ì¶”ê°€
crawler = registry.get_crawler("Coupang")  # ì¡°íšŒ
```

### 2. Strategy íŒ¨í„´ (BaseCrawler)
ê° íšŒì‚¬ë³„ë¡œ ë‹¤ë¥¸ í¬ë¡¤ë§ ì „ëµì„ êµ¬í˜„:
- URL ì¶”ì¶œ ë¡œì§ (í¬ë¡¤ëŸ¬ë³„ë¡œ ë‹¤ë¦„)
- ë°ì´í„° íŒŒì‹± (í˜ì´ì§€ êµ¬ì¡°ê°€ ë‹¤ë¦„)
- ë™ì‹œì„± ì„¤ì • (CloudFlare ë“± ì œì•½ì´ ë‹¤ë¦„)

### 3. Semaphore ê¸°ë°˜ ë™ì‹œì„± ì œì–´
ê° í¬ë¡¤ëŸ¬ì˜ `get_max_concurrent_jobs()`ì— ë”°ë¼ ë™ì‹œ ìš”ì²­ ìˆ˜ ì œí•œ:
```python
# Coupang: 2ê°œ (CloudFlare ë³´í˜¸ë¡œ ë³´ìˆ˜ì )
# Woowahan: 3ê°œ (ì•ˆì •ì )
# Kakao: 3ê°œ (ì¼ë°˜ì )
```

### 4. í•˜ì´ë¸Œë¦¬ë“œ CloudFlare ìš°íšŒ
- **ëª©ë¡ í˜ì´ì§€**: `cloudscraper` (ìš”ì²­ ê¸°ë°˜)
- **ìƒì„¸ í˜ì´ì§€**: `playwright-stealth` (ë¸Œë¼ìš°ì € ì—ë®¬ë ˆì´ì…˜)
- **íš¨ê³¼**: CloudFlareì˜ JavaScript ì±Œë¦°ì§€ + ë´‡ ê°ì§€ ëª¨ë‘ ìš°íšŒ

### 5. ë°ì´í„° ì €ì¥ ì „ëµ
3ê°€ì§€ í˜•ì‹ìœ¼ë¡œ ë³‘ë ¬ ì €ì¥:
- **HTML**: ì›ë³¸ í˜ì´ì§€ (ê²€ìƒ‰/ë¶„ì„ìš©)
- **JSON**: êµ¬ì¡°í™”ëœ ë©”íƒ€ë°ì´í„° (ì²˜ë¦¬ìš©)
- **PDF**: ì‹œê°ì  ì¦ê±° (ë³´ê´€ìš©)

## ì„±ëŠ¥ íŠ¹ì„±

### ë™ì‹œì„± ëª¨ë¸
- **ë¹„ë™ê¸°/ë³‘ë ¬**: asyncio + asyncio.Semaphore
- **ìŠ¤ë ˆë“œ ì‚¬ìš©**: `asyncio.to_thread()` for I/O ì‘ì—…
- **ì˜¤ë²„í—¤ë“œ**: ë¯¸ë‹ˆë©€ (ì´ë²¤íŠ¸ ë£¨í”„ ê¸°ë°˜)

### ì²˜ë¦¬ ì‹œê°„ ì˜ˆìƒ
- **Woowahan** (20ê°œ): ~50ì´ˆ
- **Coupang** (3ê°œ): ~65ì´ˆ
- ì´ í¬ë¡¤ë§ + PDF ìº¡ì²˜ í¬í•¨

### ë©”ëª¨ë¦¬ ì‚¬ìš©
- **ë¸Œë¼ìš°ì € ì¸ìŠ¤í„´ìŠ¤**: íšŒì‚¬ë‹¹ 1ê°œ (ì»¨í…ìŠ¤íŠ¸ ì¬ì‚¬ìš©)
- **í˜ì´ì§€**: Semaphoreë¡œ ë™ì‹œ ìƒì„± ì œì–´
- **PDF ë©”ëª¨ë¦¬**: streamingìœ¼ë¡œ ìµœì í™”

## ì—ëŸ¬ ì²˜ë¦¬

### í¬ë¡¤ë§ ì‹¤íŒ¨ ì‹œë‚˜ë¦¬ì˜¤
1. **URL ì¶”ì¶œ ì‹¤íŒ¨**: í•´ë‹¹ íšŒì‚¬ ì „ì²´ ìŠ¤í‚µ
2. **ê°œë³„ ê³µê³  íŒŒì‹± ì‹¤íŒ¨**: í•´ë‹¹ ê³µê³ ë§Œ ìŠ¤í‚µ, ë‚˜ë¨¸ì§€ ê³„ì†
3. **ì´ë¯¸ì§€ ìº¡ì²˜ ì‹¤íŒ¨**: HTML/JSONì€ ì €ì¥, ì´ë¯¸ì§€ë§Œ ìŠ¤í‚µ
4. **íŒŒì¼ ì €ì¥ ì‹¤íŒ¨**: ë¡œê·¸ë§Œ ê¸°ë¡, ê³„ì† ì§„í–‰

### ì˜ˆì™¸ ì²˜ë¦¬
- ëª¨ë“  async ì‘ì—…ì€ `try-except`ë¡œ ê°ì‹¸ì§
- ì—ëŸ¬ëŠ” `error_logs` ë°°ì—´ì— ìˆ˜ì§‘
- ë¶€ë¶„ ì‹¤íŒ¨ë„ ì„±ê³µìœ¼ë¡œ ê°„ì£¼ (ì¼ë¶€ ë°ì´í„°ëŠ” ì €ì¥ë¨)

## í˜„ì¬ ê°œë°œ ìƒíƒœ (Development Status)

### ì™„ë£Œëœ í¬ë¡¤ëŸ¬
- âœ… **Toss** (í† ìŠ¤): HTML + JSON ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ
  - íŠ¹ì´ì‚¬í•­: ê°™ì€ í¬ì§€ì…˜ì— ëŒ€í•´ íƒ­ìœ¼ë¡œ ê³„ì—´ì‚¬ë¥¼ êµ¬ë¶„í•˜ëŠ” ê²½ìš°ê°€ ìˆì–´ HTMLê³¼ ìŠ¤í¬ë¦°ìƒ· ê°œìˆ˜ ì°¨ì´ê°€ ë°œìƒí•  ìˆ˜ ìˆìŒ
  - ì˜ˆ: 1ê°œ í¬ì§€ì…˜ì´ 5ê°œ ê³„ì—´ì‚¬ íƒ­ìœ¼ë¡œ í‘œí˜„ë˜ëŠ” ê²½ìš° HTML 5ê°œ, í¬ì§€ì…˜ 1ê°œ

- âœ… **LG** (LG ì±„ìš©ì‚¬ì´íŠ¸): HTML + JSON ì €ì¥ ì™„ë£Œ
  - ê³¼ê±° í¬ë¡¤ë§ ê²°ê³¼: ë°ì´í„° ë³´ì¡´

- âœ… **Coupang** (ì¿ íŒ¡): HTML + JSON ì €ì¥ (ìŠ¤í¬ë¦°ìƒ· ì œì™¸)
  - ì œì•½ì‚¬í•­: ë…¸íŠ¸ë¶ ìš©ëŸ‰ ë¶€ì¡± ë° CloudFlare ë¸”ë¡œí‚¹ìœ¼ë¡œ ìŠ¤í¬ë¦°ìƒ· ìˆ˜ì§‘ ë¶ˆê°€

- âœ… **Hanwha, Hyundai, Naver, Kakao, KT, Posco, DaangN, WooWahan, Line**: ê¸°ë³¸ êµ¬í˜„ ì™„ë£Œ

### ì§„í–‰ ì¤‘ì¸ ì‘ì—…
- ğŸ”§ **ìŠ¤í¬ë¦°ìƒ· ìº¡ì²˜** (PlaywrightCaptureAgent)
  - Toss: ìŠ¤í¬ë¦°ìƒ· êµ¬í˜„ ì‹¤íŒ¨ (í˜„ì¬ ì¡°ì‚¬ ì¤‘)
  - Coupang: ìŠ¤í¬ë¦°ìƒ· ë¶ˆê°€ (ìš©ëŸ‰ ë¶€ì¡± + CloudFlare ë¸”ë¡)

### í–¥í›„ ê°œì„  ì‚¬í•­
- ğŸ“‹ **ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™**: í˜„ì¬ íŒŒì¼ ê¸°ë°˜ ì €ì¥ë§Œ ì§€ì›
  - DB ì—°ê²° í•„ìš” (PostgreSQL/MySQL ë“±)
  - ì •ê¸°ì  í¬ë¡¤ë§ ì‹œ DBì— ì—†ëŠ” ê³µê³ ë§Œ ì„ íƒì ìœ¼ë¡œ ìˆ˜ì§‘í•˜ëŠ” ì—…ë°ì´íŠ¸ ë¡œì§ ì¶”ê°€ í•„ìš”

- ğŸ” **OCR ì •í™•ë„ ê°œì„ **: ì „ì²´ í˜ì´ì§€ ìŠ¤í¬ë¦°ìƒ· ì‹œ ê¸€ì í¬ê¸° ê°ì†Œë¡œ OCR ì •í™•ë„ ì €í•˜ ê°€ëŠ¥
  - í•´ê²°ì•ˆ: Viewport í¬ê¸° ìµœì í™”, ë¶€ë¶„ ìº¡ì²˜ ë“± ê³ ë ¤

- ğŸ‘” **ê³ ìš©í˜•íƒœ í•„í„°ë§**: í˜„ì¬ ê²½ë ¥/ì‹ ì… í¬ì§€ì…˜ë§Œ ìˆ˜ì§‘, ê³„ì•½ì§/ì¸í„´ ë“±ì€ ì œì™¸ ì „
  - í–¥í›„: í¬ì§€ì…˜ íƒ€ì…ë³„ í•„í„°ë§ ë¡œì§ ì¶”ê°€ í•„ìš”

- ğŸ“Š **ê³¼ê±° í¬ë¡¤ë§ ë°ì´í„°**: ëª¨ë“  ì´ì „ ìˆ˜ì§‘ ê²°ê³¼ë¥¼ `data/` í´ë”ì— ë³´ì¡´í•¨

## íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ë¬¸ì œ 1: "CloudFlare ë³´í˜¸ë¡œ ì¸í•´ ì ‘ê·¼ ë¶ˆê°€"
**ì¦ìƒ**: `Error 1020: Access Denied` ë˜ëŠ” `Error 1009: Country Restricted`
**ì›ì¸**: ì›¹ì‚¬ì´íŠ¸ê°€ CloudFlare ë³´í˜¸ë¥¼ í•˜ê³  ìˆìŒ
**í•´ê²°ì±…**:
1. `get_max_concurrent_jobs()` ê°’ì„ ë” ì‘ê²Œ ì„¤ì • (ë³´ìˆ˜ì ìœ¼ë¡œ)
2. `get_wait_time()` ê°’ì„ ë” í¬ê²Œ ì„¤ì • (ë¡œë“œ ì‹œê°„ ì¦ê°€)
3. playwright-stealthê°€ ì ìš©ë˜ëŠ”ì§€ í™•ì¸: `crawlers/base_crawler.py` ì°¸ê³ 

```python
def get_max_concurrent_jobs(self) -> int:
    return 2  # CloudFlare ëŒ€ë¹„ ë³´ìˆ˜ì  ì„¤ì •

def get_wait_time(self) -> int:
    return 5  # ë¡œë“œ ëŒ€ê¸°ì‹œê°„ ì¦ê°€
```

### ë¬¸ì œ 2: "Timeout ì—ëŸ¬"
**ì¦ìƒ**: `Timeout waiting for page load` ë˜ëŠ” `Navigation timeout`
**ì›ì¸**: í˜ì´ì§€ ë¡œë“œê°€ ë„ˆë¬´ ì˜¤ë˜ ê±¸ë¦¼
**í•´ê²°ì±…**:
```python
# 1. Timeout ê°’ ì¦ê°€
image_bytes = await capture_agent.capture_as_image(
    url=url,
    timeout=90000  # ê¸°ë³¸ê°’: 60000ms (1ë¶„), 1.5ë¶„ìœ¼ë¡œ ì¦ê°€
)

# 2. wait_time ì¦ê°€
capture_as_image(..., wait_time=10)  # ë¡œë“œ í›„ 10ì´ˆ ëŒ€ê¸°

# 3. ë„¤íŠ¸ì›Œí¬ ìƒí™© ê°œì„  (í”„ë¡ì‹œ ì‚¬ìš©, ì‹œê°„ ë³€ê²½)
```

### ë¬¸ì œ 3: "ë©”ëª¨ë¦¬ ë¶€ì¡± (Out of Memory)"
**ì¦ìƒ**: `MemoryError` ë˜ëŠ” ì‹œìŠ¤í…œì´ ì‘ë‹µ ì—†ìŒ
**ì›ì¸**: ë™ì‹œ ì²˜ë¦¬ ê³µê³ ê°€ ë„ˆë¬´ ë§ìŒ, PNG ì´ë¯¸ì§€ê°€ í¼
**í•´ê²°ì±…**:
```python
# 1. ë™ì‹œì„± ê°ì†Œ
registry.register(MyCrawler())  # get_max_concurrent_jobs() = 2

# 2. JPEG í¬ë§· ì‚¬ìš© (PNG ë³´ë‹¤ 90% ë” ì‘ìŒ)
image_bytes = await capture_agent.capture_as_image(
    url=url,
    image_format="jpeg"  # PNG ëŒ€ì‹  JPEG ì‚¬ìš©
)

# 3. ìµœëŒ€ ê³µê³  ìˆ˜ ì œí•œ
result = await orchestrator.crawl_company("MyCompany", max_jobs=5)
```

### ë¬¸ì œ 4: "ì´ë¯¸ì§€ê°€ ì €ì¥ë˜ì§€ ì•ŠìŒ"
**ì¦ìƒ**: HTML/JSONì€ ì €ì¥ë˜ì§€ë§Œ ì´ë¯¸ì§€ íŒŒì¼ì´ ì—†ìŒ
**ì›ì¸**: ì´ë¯¸ì§€ ìº¡ì²˜ ì‹¤íŒ¨ ë˜ëŠ” ì €ì¥ì†Œ ê¶Œí•œ ë¬¸ì œ
**í™•ì¸ ì‚¬í•­**:
```bash
# 1. ë””ë ‰í† ë¦¬ ìƒì„± í™•ì¸
ls -la data/screenshots/

# 2. íŒŒì¼ ì‹œìŠ¤í…œ ê¶Œí•œ í™•ì¸
ls -la data/

# 3. ë¡œê·¸ í™•ì¸
grep "Image captured" crawler.log
grep "Failed to capture image" crawler.log
```

### ë¬¸ì œ 5: "ì¤‘ë³µëœ ë°ì´í„° ì €ì¥"
**ì¦ìƒ**: ê°™ì€ ê³µê³ ê°€ ì—¬ëŸ¬ ë²ˆ ì €ì¥ë¨
**ì›ì¸**: í¬ë¡¤ëŸ¬ê°€ ì¤‘ë³µëœ URLì„ ì¶”ì¶œí•¨
**í•´ê²°ì±…**:
```python
# BaseCrawlerì—ì„œ ì¤‘ë³µ ì œê±°
async def extract_job_urls(self, page: Page) -> List[Dict[str, str]]:
    job_urls = []
    seen_urls = set()  # ì¤‘ë³µ ì¶”ì 

    for url in raw_urls:
        normalized = url.rstrip('/')  # ì •ê·œí™”
        if normalized not in seen_urls:
            job_urls.append({"url": normalized, "job_id": ...})
            seen_urls.add(normalized)

    return job_urls
```

## ìì£¼ ë¬»ëŠ” ì§ˆë¬¸ (FAQ)

### Q1: PNGì™€ JPEG í˜•ì‹ ì¤‘ ì–´ë–¤ ê²ƒì„ ì‚¬ìš©í•´ì•¼ í•˜ë‚˜?
**A**:
- **PNG** (ê¸°ë³¸ê°’): ì†ì‹¤ ì—†ëŠ” ê³ í’ˆì§ˆ (~2.4MB/ê°œ)
  - ì¥ì : ì••ì¶•í•˜ì§€ ì•Šì€ ì •í™•í•œ ì‹œê°í™”
  - ë‹¨ì : íŒŒì¼ í¬ê¸° í¼, ì €ì¥ì†Œ ë§ì´ í•„ìš”
- **JPEG**: ì†ì‹¤ ì••ì¶• (~300KB/ê°œ, ~90% ë” ì‘ìŒ)
  - ì¥ì : íŒŒì¼ í¬ê¸° ì‘ìŒ, ë¹ ë¥¸ ì €ì¥
  - ë‹¨ì : ì•½ê°„ì˜ í’ˆì§ˆ ì†ì‹¤

**ì¶”ì²œ**: ì €ì¥ì†Œ ì œì•½ì´ ì—†ìœ¼ë©´ PNG, ì €ì¥ì†Œ ì œì•½ì´ ìˆìœ¼ë©´ JPEG ì‚¬ìš©

### Q2: ìƒˆ íšŒì‚¬ë¥¼ ì¶”ê°€í•˜ë ¤ë©´ ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜?
**A**: 5ë‹¨ê³„ë¡œ ì§„í–‰:
1. `crawlers/` ë””ë ‰í† ë¦¬ì— ìƒˆ íŒŒì¼ ìƒì„± (ì˜ˆ: `amazon.py`)
2. `BaseCrawler` ìƒì†í•˜ëŠ” í´ë˜ìŠ¤ ì‘ì„±
3. í•„ìˆ˜ ë©”ì„œë“œ êµ¬í˜„ (ìœ„ì˜ "ìƒˆë¡œìš´ íšŒì‚¬ í¬ë¡¤ëŸ¬ ì¶”ê°€í•˜ê¸°" ì„¹ì…˜ ì°¸ê³ )
4. `crawlers/__init__.py`ì—ì„œ import
5. ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸ì—ì„œ ë ˆì§€ìŠ¤íŠ¸ë¦¬ì— ë“±ë¡

### Q3: í¬ë¡¤ë§í•œ ë°ì´í„°ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•  ìˆ˜ ìˆë‚˜?
**A**: ê°€ëŠ¥í•©ë‹ˆë‹¤! ë‘ ê°€ì§€ ë°©ë²•:
1. **StorageAgent ìƒì†**:
```python
class DatabaseStorageAgent(StorageAgent):
    def save_json_locally(self, job, company, job_id, subfolder):
        # íŒŒì¼ ì €ì¥
        local_path = super().save_json_locally(job, company, job_id, subfolder)
        # + DB ì €ì¥
        self.db.insert("jobs", job)
        return local_path
```

2. **ê²°ê³¼ í›„ì²˜ë¦¬**:
```python
result = await orchestrator.crawl_company("Coupang")
for job in result['job_listings']:
    database.insert(job)
```

### Q4: í”„ë¡ì‹œë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë‚˜?
**A**: Playwrightì—ì„œ ì§€ì›í•©ë‹ˆë‹¤:
```python
browser = await p.chromium.launch(
    proxy="http://proxy.example.com:8080"
)
```

### Q5: í¬ë¡¤ëŸ¬ë¥¼ ìŠ¤ì¼€ì¤„ë§í•´ì„œ ë§¤ì¼ ì‹¤í–‰í•  ìˆ˜ ìˆë‚˜?
**A**: APSchedulerë‚˜ Celery ì‚¬ìš©:
```python
from apscheduler.schedulers.asyncio import AsyncIOScheduler

scheduler = AsyncIOScheduler()
scheduler.add_job(crawl_job, 'cron', hour=9)  # ë§¤ì¼ 9ì‹œ ì‹¤í–‰
```

### Q6: ë¡œê·¸ë¥¼ íŒŒì¼ì— ì €ì¥í•˜ê³  ì‹¶ì–´ìš”
**A**: ë¡œê¹… ì„¤ì •:
```python
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('crawler.log'),
        logging.StreamHandler()
    ]
)
```

## í™•ì¥ ê°€ëŠ¥ì„±

### ë²¡í„° ì„ë² ë”©
```python
orchestrator = AsyncPlaywrightOrchestrator(
    ...,
    use_vector_embedding=True  # í™œì„±í™”
)
# JobPostingData.vector_embeddingì— ì €ì¥ë¨
```

### ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™
StorageAgent ìƒì† í›„ `save_to_db()` ë©”ì„œë“œ ì¶”ê°€:
```python
class DatabaseStorageAgent(StorageAgent):
    def save_json_locally(self, ...):
        # DBì—ë„ ì €ì¥
```

### ìŠ¤ì¼€ì¤„ë§
Celery, APScheduler ë“±ê³¼ í†µí•© ê°€ëŠ¥ (asyncio í˜¸í™˜)

## ë¼ì´ì„ ìŠ¤

MIT License

## ì§€ì›

ë¬¸ì œê°€ ë°œìƒí•˜ë©´ ì´ìŠˆë¥¼ ë“±ë¡í•˜ê±°ë‚˜ ë¡œê·¸ íŒŒì¼(`crawler.log`)ì„ í™•ì¸í•˜ì„¸ìš”.
